## Where do text models currently have a major deficiency?

Text models not good at generating correct responses, it is not reliable, so using text model is a dangerous. Text generation models will always be technologically a bit ahead of models for recognizing automatically generated text. 

Because of this issues, deep learning used not as an entirely process, using this system with human interaction could make better outputs.

## What are possible negative societal implications of text generation models?

Text generation models can be used in troll attacks in socical media to spread disinformation, create unrest etc. Actually it is very good at it.

## In situations where a model might make mistakes, and those mistakes could be harmful, what is good alternative for automating a process?

Similarly what we say in text models, manual process can be use in that situations. 

Manual process; we can run a model in parallel, humans check all predictions.

Limited Scope Deployment (rather than rolling out whole)

-Careful Human Supervision

-Time or geography Limited

Suppose we create a model that detects vehicles exceeding the speed limit. Human can supervise this system and we can geographically limit our scope before the using this model in all country.

## What kind of tabular data is deep learning particularly good at?

Deep learning does greatly increase the variety of colmns that you can include, for example, columns containing natural language and high cardinality categorical columns (discrete choices, such as zip core od product ID)

Take more time to learn in tabular data but libraries such as RAPIDS can solve this problem. 

https://developer.nvidia.com/rapids

## What's a key downside of directly using a deep learning model for recommendations systems?

Recommendations systems very alike tabuldar data. The goal is use the collaborative filtering to fill in the matrix gap in user. Biggest problem is if you buy a apple watch and ipad, and second person buys a apple watch ipad and mug, engine will recommend that mug.

DL models are good at high cardinality categorical variables and they are quite good at it. The problem is that, engine will tell, which product you might like,rather than what recommendations would be helpful for a user.

Imagine you are in the bookstore, the bookseller may suggest you george orwell, but the system will suggest other books by albert camus.

## What are the steps of the Drivetrain Approach?

1 - Defined Objective (What outcome am i trying to achieve)

2 - Levers (What inputs can we control)

3- Data (What inputs can we collect)

4- Models (How they levers influence the objective)

## How do the steps of the Drivetrain Approach map to a recommendation system?

1- Defined Objective, making additional sales by suprising and delihting the customer

2 - Ranking the recommendations

3 - Data, new data must be collected to generate recommendations that will cause new sales. This will require conducting many randomized experiments in order to collect data about a wide range of recoemmendations for a wide range customers. This step that few organizations take, without it you don't have the information you need to optimize recommendations based on your true objective (more sales)

4 - Models, you could build two models for purchase probabilities, conditional on seeing
or not seeing a recommendation. The difference between these two probabilities is a
utility function for a given recommendation to a customer. It will be low in cases
where the algorithm recommends a familiar book that the customer has already
rejected (both components are small) or a book that they would have bought even
without the recommendation (both components are large and cancel each other out).

## What is dataloaders?

 It's great video for understanding dataloaders with epoch. https://www.youtube.com/watch?v=zN49HdDxHi8
 
 ## 4 things we need to tell fastai to use it
 
 1 - What kinds of data we are working with
 
 2 - How to get the list of items
 
 3- How to label these items 
 
 4- How to create the validation set
 
 ## What does the splitter parameter to DataBlock do?
 
 We want to split our data same split each time we run this notebook, so we use the random seed and percentage ;
 
 RandomSplitter(valid_pct=0.2, seed=42)
 
 ## How do we ensure a random split always gives the same validation set?
 
 By setting seeds.
 
## What letters are often used to signify the independent and dependent variables?

The independent variable is often referred to as x, and the dependent variable is often
referred to as y.

## Whatâ€™s the difference between the crop, pad, and squish resize approaches? When might you choose one over the others?


Resize crops the images to fit a square shape of the size requested, using
the full width or height. This can result in losing some important details. For example; bear back can be important for the identify the bear kind, so that losing these details bad for the model accuracy.

Alternatively,
you can ask fastai to pad the images with zeros (black), or squish/stretch them: If we squish or
stretch the images, they end up as unrealistic shapes, leading to a model that learns
that things look different from how they actually are, which we would expect to result
in lower accuracy in real world. If we pad the images, we have a whole lot of
empty space, which is just wasted computation for our model and results in a lower
effective resolution for the part of the image we actually use.

Instead, what we normally do in practice is to randomly select part of the image and
then crop to just that part. On each epoch (which is one complete pass through all of
our images in the dataset), we randomly select a different part of each image. This
means that our model can learn to focus on, and recognize, different features in our
images. It also reflects how images work in the real world: different photos of the
same thing may be framed in slightly different ways.

Replace Resize with RandomResizedCrop, which is
the transform that provides the behavior just described. The most important parameter
to pass in is min_scale, which determines how much of the image to select at
minimum each time:


## What is data augmentation? Why it is needed?

Our training images could be not reflect the real world situation, for example our bear detection system trained with bears in the bing image services but in real world, Part of the bear may be behind the bushes ot they can come at pitch black. So our object when we do data augmentation is bringing training set closer to real world.


